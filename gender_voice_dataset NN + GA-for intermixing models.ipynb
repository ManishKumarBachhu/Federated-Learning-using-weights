{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1016 18:16:22.641939 12692 __init__.py:687] \n",
      "\n",
      "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\n",
      "\n",
      "  Please upgrade your code to TensorFlow 2.0:\n",
      "    * https://www.tensorflow.org/beta/guide/migration_guide\n",
      "\n",
      "  Or install the latest stable TensorFlow 1.X release:\n",
      "    * `pip install -U \"tensorflow==1.*\"`\n",
      "\n",
      "  Otherwise your code may be broken by the change.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "df = pd.read_csv(\"data/gender_voice_dataset.csv\")\n",
    "INPUT_SHAPE = 20\n",
    "LABEL = 'label'\n",
    "SPLIT_SIZE = 10\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "x = df.drop([LABEL], 1)\n",
    "y = df[LABEL]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "#print(y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, x_test, y, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "for i in range(0,SPLIT_SIZE):\n",
    "    X_train.append(x[int((i*len(x)/SPLIT_SIZE)):(int((i+1)*len(x)/SPLIT_SIZE))])\n",
    "    Y_train.append(y[(int(i*len(x)/SPLIT_SIZE)):(int((i+1)*len(x)/SPLIT_SIZE))])\n",
    "    \n",
    "#print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build() :\n",
    "    model1 = tf.keras.models.Sequential()\n",
    "    model1.add(tf.keras.layers.Dense(10, input_shape = (INPUT_SHAPE,), activation='relu', use_bias = False))\n",
    "    model1.add(tf.keras.layers.Dense(10, activation='relu', use_bias = False))\n",
    "    model1.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model1.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 18:16:23.920244 12692 deprecation.py:506] From C:\\Users\\Manish\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1633: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W1016 18:16:24.124957 12692 deprecation.py:323] From C:\\Users\\Manish\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1016 18:16:24.616383 12692 deprecation.py:323] From C:\\Users\\Manish\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py:468: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 253 samples\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 0s 595us/sample - loss: 1.2708 - acc: 0.5138\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 55us/sample - loss: 0.9992 - acc: 0.5138\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 77us/sample - loss: 0.8019 - acc: 0.5138\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 79us/sample - loss: 0.7296 - acc: 0.4664\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 91us/sample - loss: 0.7557 - acc: 0.4585\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 83us/sample - loss: 0.7059 - acc: 0.4783\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 62us/sample - loss: 0.7046 - acc: 0.4783\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 78us/sample - loss: 0.6993 - acc: 0.4901\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 44us/sample - loss: 0.7027 - acc: 0.4822\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 73us/sample - loss: 0.6901 - acc: 0.4862\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 70us/sample - loss: 0.7049 - acc: 0.4822\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 58us/sample - loss: 0.7090 - acc: 0.4980\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 86us/sample - loss: 0.7032 - acc: 0.5138\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - ETA: 0s - loss: 0.6844 - acc: 0.437 - 0s 86us/sample - loss: 0.6921 - acc: 0.5099\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 62us/sample - loss: 0.7010 - acc: 0.5257\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 77us/sample - loss: 0.6935 - acc: 0.5020\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 64us/sample - loss: 0.7041 - acc: 0.5059\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 72us/sample - loss: 0.6886 - acc: 0.5336\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 60us/sample - loss: 0.6853 - acc: 0.5217\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 75us/sample - loss: 0.6851 - acc: 0.5415\n",
      "Train on 253 samples\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 0s 465us/sample - loss: 1.0160 - acc: 0.4822\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 78us/sample - loss: 0.9325 - acc: 0.4783\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 88us/sample - loss: 0.8565 - acc: 0.4822\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 96us/sample - loss: 0.8063 - acc: 0.4783\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 63us/sample - loss: 0.7447 - acc: 0.4783\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 89us/sample - loss: 0.6999 - acc: 0.5257\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 108us/sample - loss: 0.6964 - acc: 0.5415\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 86us/sample - loss: 0.6848 - acc: 0.5692\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 99us/sample - loss: 0.6742 - acc: 0.5810\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 79us/sample - loss: 0.6633 - acc: 0.5929\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 65us/sample - loss: 0.6541 - acc: 0.6008\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 81us/sample - loss: 0.6523 - acc: 0.6047\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 63us/sample - loss: 0.6499 - acc: 0.6245\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 90us/sample - loss: 0.6393 - acc: 0.6403\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 89us/sample - loss: 0.6380 - acc: 0.6601\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 105us/sample - loss: 0.6435 - acc: 0.6482\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 95us/sample - loss: 0.6376 - acc: 0.6601\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 92us/sample - loss: 0.6221 - acc: 0.6640\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 90us/sample - loss: 0.6263 - acc: 0.7115\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 63us/sample - loss: 0.6162 - acc: 0.7036\n",
      "Train on 254 samples\n",
      "Epoch 1/20\n",
      "254/254 [==============================] - 0s 547us/sample - loss: 8.3219 - acc: 0.5157\n",
      "Epoch 2/20\n",
      "254/254 [==============================] - 0s 85us/sample - loss: 7.3246 - acc: 0.5157\n",
      "Epoch 3/20\n",
      "254/254 [==============================] - 0s 107us/sample - loss: 6.2642 - acc: 0.5157\n",
      "Epoch 4/20\n",
      "254/254 [==============================] - 0s 100us/sample - loss: 5.5421 - acc: 0.5276\n",
      "Epoch 5/20\n",
      "254/254 [==============================] - 0s 99us/sample - loss: 4.7433 - acc: 0.5039\n",
      "Epoch 6/20\n",
      "254/254 [==============================] - 0s 100us/sample - loss: 4.0262 - acc: 0.5157\n",
      "Epoch 7/20\n",
      "254/254 [==============================] - 0s 72us/sample - loss: 3.6314 - acc: 0.5157\n",
      "Epoch 8/20\n",
      "254/254 [==============================] - 0s 98us/sample - loss: 3.2714 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "254/254 [==============================] - 0s 115us/sample - loss: 2.9722 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "254/254 [==============================] - 0s 137us/sample - loss: 2.6670 - acc: 0.5039\n",
      "Epoch 11/20\n",
      "254/254 [==============================] - 0s 98us/sample - loss: 2.3318 - acc: 0.4961\n",
      "Epoch 12/20\n",
      "254/254 [==============================] - 0s 75us/sample - loss: 2.1079 - acc: 0.4921\n",
      "Epoch 13/20\n",
      "254/254 [==============================] - 0s 129us/sample - loss: 1.8741 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "254/254 [==============================] - 0s 89us/sample - loss: 1.6724 - acc: 0.5039\n",
      "Epoch 15/20\n",
      "254/254 [==============================] - 0s 94us/sample - loss: 1.4495 - acc: 0.5157\n",
      "Epoch 16/20\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.6649 - acc: 0.437 - 0s 99us/sample - loss: 1.2634 - acc: 0.5276\n",
      "Epoch 17/20\n",
      "254/254 [==============================] - 0s 79us/sample - loss: 1.0556 - acc: 0.5394\n",
      "Epoch 18/20\n",
      "254/254 [==============================] - 0s 108us/sample - loss: 0.9026 - acc: 0.5354\n",
      "Epoch 19/20\n",
      "254/254 [==============================] - 0s 72us/sample - loss: 0.7415 - acc: 0.5315\n",
      "Epoch 20/20\n",
      "254/254 [==============================] - 0s 79us/sample - loss: 0.6800 - acc: 0.5709\n",
      "Train on 253 samples\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 0s 621us/sample - loss: 0.8331 - acc: 0.4783\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 71us/sample - loss: 0.7448 - acc: 0.5217\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 79us/sample - loss: 0.7189 - acc: 0.5217\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 75us/sample - loss: 0.6873 - acc: 0.5257\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 70us/sample - loss: 0.6775 - acc: 0.5494\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 95us/sample - loss: 0.6835 - acc: 0.5771\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 99us/sample - loss: 0.6785 - acc: 0.5692\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 82us/sample - loss: 0.6669 - acc: 0.5771\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 80us/sample - loss: 0.6735 - acc: 0.5850\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 83us/sample - loss: 0.6673 - acc: 0.5771\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 110us/sample - loss: 0.6799 - acc: 0.5573\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 50us/sample - loss: 0.6853 - acc: 0.5573\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 82us/sample - loss: 0.6751 - acc: 0.5613\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 91us/sample - loss: 0.6561 - acc: 0.5731\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 66us/sample - loss: 0.6620 - acc: 0.5731\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 65us/sample - loss: 0.6601 - acc: 0.5731\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 87us/sample - loss: 0.6584 - acc: 0.5494\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 69us/sample - loss: 0.6533 - acc: 0.5573\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 110us/sample - loss: 0.6517 - acc: 0.5771\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 70us/sample - loss: 0.6558 - acc: 0.5494\n",
      "Train on 254 samples\n",
      "Epoch 1/20\n",
      "254/254 [==============================] - 0s 917us/sample - loss: 3.0113 - acc: 0.4567\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 0s 57us/sample - loss: 2.7501 - acc: 0.4567\n",
      "Epoch 3/20\n",
      "254/254 [==============================] - 0s 66us/sample - loss: 2.4933 - acc: 0.4567\n",
      "Epoch 4/20\n",
      "254/254 [==============================] - 0s 55us/sample - loss: 2.2573 - acc: 0.4567\n",
      "Epoch 5/20\n",
      "254/254 [==============================] - 0s 56us/sample - loss: 2.0495 - acc: 0.4567\n",
      "Epoch 6/20\n",
      "254/254 [==============================] - 0s 62us/sample - loss: 1.8775 - acc: 0.4567\n",
      "Epoch 7/20\n",
      "254/254 [==============================] - 0s 40us/sample - loss: 1.7141 - acc: 0.4567\n",
      "Epoch 8/20\n",
      "254/254 [==============================] - 0s 69us/sample - loss: 1.5881 - acc: 0.4567\n",
      "Epoch 9/20\n",
      "254/254 [==============================] - 0s 69us/sample - loss: 1.4928 - acc: 0.4567\n",
      "Epoch 10/20\n",
      "254/254 [==============================] - 0s 65us/sample - loss: 1.3912 - acc: 0.4567\n",
      "Epoch 11/20\n",
      "254/254 [==============================] - 0s 54us/sample - loss: 1.2929 - acc: 0.4606\n",
      "Epoch 12/20\n",
      "254/254 [==============================] - 0s 58us/sample - loss: 1.2429 - acc: 0.4488\n",
      "Epoch 13/20\n",
      "254/254 [==============================] - 0s 32us/sample - loss: 1.1776 - acc: 0.4764\n",
      "Epoch 14/20\n",
      "254/254 [==============================] - 0s 57us/sample - loss: 1.1199 - acc: 0.5709\n",
      "Epoch 15/20\n",
      "254/254 [==============================] - 0s 56us/sample - loss: 1.0491 - acc: 0.5787\n",
      "Epoch 16/20\n",
      "254/254 [==============================] - 0s 70us/sample - loss: 1.0170 - acc: 0.5669\n",
      "Epoch 17/20\n",
      "254/254 [==============================] - 0s 71us/sample - loss: 0.9440 - acc: 0.5630\n",
      "Epoch 18/20\n",
      "254/254 [==============================] - 0s 80us/sample - loss: 0.9008 - acc: 0.5630\n",
      "Epoch 19/20\n",
      "254/254 [==============================] - 0s 107us/sample - loss: 0.8652 - acc: 0.5394\n",
      "Epoch 20/20\n",
      "254/254 [==============================] - 0s 65us/sample - loss: 0.8101 - acc: 0.5276\n",
      "Train on 253 samples\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 0s 648us/sample - loss: 1.9292 - acc: 0.4743\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 61us/sample - loss: 1.7633 - acc: 0.4783\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 85us/sample - loss: 1.5825 - acc: 0.4822\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 74us/sample - loss: 1.3806 - acc: 0.4704\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 73us/sample - loss: 1.2541 - acc: 0.4545\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 72us/sample - loss: 1.1849 - acc: 0.4822\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 58us/sample - loss: 1.0493 - acc: 0.5099\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 62us/sample - loss: 0.9953 - acc: 0.5257\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 57us/sample - loss: 0.8886 - acc: 0.4941\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 59us/sample - loss: 0.8260 - acc: 0.5138\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 64us/sample - loss: 0.7537 - acc: 0.5296\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 60us/sample - loss: 0.7068 - acc: 0.5415\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 52us/sample - loss: 0.7373 - acc: 0.5099\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 66us/sample - loss: 0.7601 - acc: 0.5178\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 47us/sample - loss: 0.7518 - acc: 0.5257\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 60us/sample - loss: 0.7062 - acc: 0.5494\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 73us/sample - loss: 0.7131 - acc: 0.5217\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 60us/sample - loss: 0.6864 - acc: 0.5415\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 84us/sample - loss: 0.7056 - acc: 0.5455\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 69us/sample - loss: 0.6954 - acc: 0.5455\n",
      "Train on 253 samples\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 0s 637us/sample - loss: 1.0691 - acc: 0.5296\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 70us/sample - loss: 0.8478 - acc: 0.5257\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 67us/sample - loss: 0.8398 - acc: 0.5336\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 84us/sample - loss: 0.7713 - acc: 0.5573\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 74us/sample - loss: 0.7303 - acc: 0.5573\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 69us/sample - loss: 0.7003 - acc: 0.5692\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 67us/sample - loss: 0.6886 - acc: 0.5771\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 74us/sample - loss: 0.6878 - acc: 0.5889\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - ETA: 0s - loss: 0.6640 - acc: 0.625 - 0s 96us/sample - loss: 0.6839 - acc: 0.5929\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 66us/sample - loss: 0.6992 - acc: 0.5850\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 65us/sample - loss: 0.6805 - acc: 0.5850\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 70us/sample - loss: 0.7007 - acc: 0.6008\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 70us/sample - loss: 0.6961 - acc: 0.5850\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 77us/sample - loss: 0.6733 - acc: 0.5810\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 77us/sample - loss: 0.6697 - acc: 0.5810\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 72us/sample - loss: 0.6794 - acc: 0.5771\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 71us/sample - loss: 0.6640 - acc: 0.5850\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 79us/sample - loss: 0.6587 - acc: 0.5850\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 79us/sample - loss: 0.6531 - acc: 0.5771\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 69us/sample - loss: 0.6586 - acc: 0.5731\n",
      "Train on 254 samples\n",
      "Epoch 1/20\n",
      "254/254 [==============================] - 0s 771us/sample - loss: 4.5073 - acc: 0.5512\n",
      "Epoch 2/20\n",
      "254/254 [==============================] - 0s 68us/sample - loss: 4.0364 - acc: 0.5512\n",
      "Epoch 3/20\n",
      "254/254 [==============================] - 0s 76us/sample - loss: 3.5943 - acc: 0.5512\n",
      "Epoch 4/20\n",
      "254/254 [==============================] - 0s 74us/sample - loss: 3.0360 - acc: 0.5472\n",
      "Epoch 5/20\n",
      "254/254 [==============================] - 0s 80us/sample - loss: 2.6038 - acc: 0.5394\n",
      "Epoch 6/20\n",
      "254/254 [==============================] - 0s 79us/sample - loss: 2.1308 - acc: 0.5433\n",
      "Epoch 7/20\n",
      "254/254 [==============================] - 0s 73us/sample - loss: 1.7976 - acc: 0.5118\n",
      "Epoch 8/20\n",
      "254/254 [==============================] - 0s 67us/sample - loss: 1.4489 - acc: 0.5079\n",
      "Epoch 9/20\n",
      "254/254 [==============================] - 0s 73us/sample - loss: 1.2306 - acc: 0.5039\n",
      "Epoch 10/20\n",
      "254/254 [==============================] - 0s 76us/sample - loss: 1.0672 - acc: 0.4567\n",
      "Epoch 11/20\n",
      "254/254 [==============================] - 0s 94us/sample - loss: 0.9969 - acc: 0.4724\n",
      "Epoch 12/20\n",
      "254/254 [==============================] - 0s 85us/sample - loss: 0.9325 - acc: 0.4685\n",
      "Epoch 13/20\n",
      "254/254 [==============================] - 0s 68us/sample - loss: 0.9049 - acc: 0.4409\n",
      "Epoch 14/20\n",
      "254/254 [==============================] - 0s 80us/sample - loss: 0.8710 - acc: 0.4606\n",
      "Epoch 15/20\n",
      "254/254 [==============================] - 0s 84us/sample - loss: 0.8360 - acc: 0.4567\n",
      "Epoch 16/20\n",
      "254/254 [==============================] - 0s 100us/sample - loss: 0.8080 - acc: 0.4291\n",
      "Epoch 17/20\n",
      "254/254 [==============================] - 0s 83us/sample - loss: 0.7885 - acc: 0.4409\n",
      "Epoch 18/20\n",
      "254/254 [==============================] - 0s 73us/sample - loss: 0.7742 - acc: 0.4370\n",
      "Epoch 19/20\n",
      "254/254 [==============================] - 0s 72us/sample - loss: 0.7647 - acc: 0.4252\n",
      "Epoch 20/20\n",
      "254/254 [==============================] - 0s 70us/sample - loss: 0.7585 - acc: 0.4488\n",
      "Train on 253 samples\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 0s 1ms/sample - loss: 0.7567 - acc: 0.4980\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 160us/sample - loss: 0.7285 - acc: 0.5217\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 184us/sample - loss: 0.7253 - acc: 0.4704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 140us/sample - loss: 0.7284 - acc: 0.4980\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 203us/sample - loss: 0.7076 - acc: 0.5138\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 114us/sample - loss: 0.6928 - acc: 0.5217\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 112us/sample - loss: 0.6880 - acc: 0.5375\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 113us/sample - loss: 0.6886 - acc: 0.5217\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 118us/sample - loss: 0.6852 - acc: 0.5257\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 49us/sample - loss: 0.6780 - acc: 0.5336\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 66us/sample - loss: 0.6759 - acc: 0.5494\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 63us/sample - loss: 0.6740 - acc: 0.5652\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 70us/sample - loss: 0.6726 - acc: 0.5889\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 73us/sample - loss: 0.6732 - acc: 0.5415\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 65us/sample - loss: 0.6716 - acc: 0.6126\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 57us/sample - loss: 0.6667 - acc: 0.6087\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 51us/sample - loss: 0.6658 - acc: 0.6443\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 52us/sample - loss: 0.6602 - acc: 0.6324\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 59us/sample - loss: 0.6598 - acc: 0.6443\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 61us/sample - loss: 0.6578 - acc: 0.6443\n",
      "Train on 254 samples\n",
      "Epoch 1/20\n",
      "254/254 [==============================] - 0s 790us/sample - loss: 2.1622 - acc: 0.4331\n",
      "Epoch 2/20\n",
      "254/254 [==============================] - 0s 65us/sample - loss: 1.6499 - acc: 0.4331\n",
      "Epoch 3/20\n",
      "254/254 [==============================] - 0s 72us/sample - loss: 1.2852 - acc: 0.4252\n",
      "Epoch 4/20\n",
      "254/254 [==============================] - 0s 67us/sample - loss: 0.9310 - acc: 0.4961\n",
      "Epoch 5/20\n",
      "254/254 [==============================] - 0s 76us/sample - loss: 0.7250 - acc: 0.5236\n",
      "Epoch 6/20\n",
      "254/254 [==============================] - 0s 86us/sample - loss: 0.7233 - acc: 0.5039\n",
      "Epoch 7/20\n",
      "254/254 [==============================] - 0s 77us/sample - loss: 0.7032 - acc: 0.4961\n",
      "Epoch 8/20\n",
      "254/254 [==============================] - 0s 90us/sample - loss: 0.6822 - acc: 0.5118\n",
      "Epoch 9/20\n",
      "254/254 [==============================] - 0s 75us/sample - loss: 0.6693 - acc: 0.5118\n",
      "Epoch 10/20\n",
      "254/254 [==============================] - 0s 78us/sample - loss: 0.6662 - acc: 0.5197\n",
      "Epoch 11/20\n",
      "254/254 [==============================] - 0s 72us/sample - loss: 0.6625 - acc: 0.5394\n",
      "Epoch 12/20\n",
      "254/254 [==============================] - 0s 70us/sample - loss: 0.6593 - acc: 0.5394\n",
      "Epoch 13/20\n",
      "254/254 [==============================] - 0s 86us/sample - loss: 0.6585 - acc: 0.5276\n",
      "Epoch 14/20\n",
      "254/254 [==============================] - 0s 62us/sample - loss: 0.6578 - acc: 0.5433\n",
      "Epoch 15/20\n",
      "254/254 [==============================] - 0s 59us/sample - loss: 0.6582 - acc: 0.5354\n",
      "Epoch 16/20\n",
      "254/254 [==============================] - 0s 76us/sample - loss: 0.6564 - acc: 0.5354\n",
      "Epoch 17/20\n",
      "254/254 [==============================] - 0s 73us/sample - loss: 0.6572 - acc: 0.5512\n",
      "Epoch 18/20\n",
      "254/254 [==============================] - 0s 60us/sample - loss: 0.6566 - acc: 0.5394\n",
      "Epoch 19/20\n",
      "254/254 [==============================] - 0s 74us/sample - loss: 0.6550 - acc: 0.5512\n",
      "Epoch 20/20\n",
      "254/254 [==============================] - 0s 63us/sample - loss: 0.6544 - acc: 0.5472\n",
      "634/634 [==============================] - 0s 363us/sample - loss: 0.6836 - acc: 0.5268\n",
      "634/634 [==============================] - 0s 226us/sample - loss: 0.6506 - acc: 0.6498\n",
      "634/634 [==============================] - 0s 190us/sample - loss: 0.6867 - acc: 0.5300\n",
      "634/634 [==============================] - 0s 188us/sample - loss: 0.6862 - acc: 0.5063\n",
      "634/634 [==============================] - 0s 316us/sample - loss: 0.8698 - acc: 0.4905\n",
      "634/634 [==============================] - 0s 235us/sample - loss: 0.6867 - acc: 0.5252\n",
      "634/634 [==============================] - 0s 231us/sample - loss: 0.6949 - acc: 0.5016\n",
      "634/634 [==============================] - 0s 220us/sample - loss: 0.7397 - acc: 0.4558\n",
      "634/634 [==============================] - 0s 211us/sample - loss: 0.7155 - acc: 0.6262\n",
      "634/634 [==============================] - 0s 186us/sample - loss: 0.6765 - acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "metrics = []\n",
    "for i in range (0,SPLIT_SIZE):\n",
    "    models.append(model_build())\n",
    "    models[i].fit(X_train[i], Y_train[i], epochs = EPOCHS)\n",
    "\n",
    "for i in range(0,SPLIT_SIZE):\n",
    "    metrics.append(models[i].evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52681386\n",
      "0.64984226\n",
      "0.52996844\n",
      "0.50630915\n",
      "0.49053627\n",
      "0.5252366\n",
      "0.50157726\n",
      "0.45583597\n",
      "0.626183\n",
      "0.5\n",
      "0.5312302798032761\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in metrics:\n",
    "    print(i[1])\n",
    "    sum = sum + i[1]\n",
    "\n",
    "average = sum/SPLIT_SIZE\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634/634 [==============================] - 0s 199us/sample - loss: 0.6885 - acc: 0.5252\n",
      "0.5252366\n",
      "0.688502864130664\n"
     ]
    }
   ],
   "source": [
    "m = models[0].get_weights()\n",
    "for num in range(1,SPLIT_SIZE):\n",
    "    a = models[num].get_weights()\n",
    "    m = numpy.add(m,a)\n",
    "m /= SPLIT_SIZE\n",
    "model = model_build()\n",
    "model.set_weights(m)\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(acc)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 2, 0, 5, 3, 6, 9, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "list1 = [0,1,2,3,4,5,6,7,8,9]\n",
    "def metricsSort(val):\n",
    "    return metrics[val][1]\n",
    "list1.sort(key = metricsSort, reverse = True)\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "2\n",
      "0\n",
      "5\n",
      "False\n",
      "634/634 [==============================] - 0s 219us/sample - loss: 0.7048 - acc: 0.5252\n",
      "0.5252366\n",
      "0.7048063389134331\n"
     ]
    }
   ],
   "source": [
    "#Selecting Top 5 individually performing models\n",
    "m2 = models[list1[0]].get_weights()\n",
    "print(list1[0])\n",
    "for i in list1[1:5]:\n",
    "    a = models[i].get_weights()\n",
    "    m2 = numpy.add(m2,a)\n",
    "    print(i)\n",
    "\n",
    "m2 /= 5\n",
    "print(all([numpy.allclose(x, y) for x, y in zip(m, m2)]))\n",
    "model2 = model_build()\n",
    "model2.set_weights(m2)\n",
    "loss, acc = model2.evaluate(x_test, y_test)\n",
    "print(acc)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "6\n",
      "9\n",
      "4\n",
      "7\n",
      "False\n",
      "634/634 [==============================] - 0s 199us/sample - loss: 0.6922 - acc: 0.4968\n",
      "0.49684542\n",
      "0.6921681694051821\n"
     ]
    }
   ],
   "source": [
    "#Selecting Lowest 5 individually performing models\n",
    "m3 = models[list1[0]].get_weights()\n",
    "print(list1[5])\n",
    "for i in list1[6:10]:\n",
    "    a = models[i].get_weights()\n",
    "    m3 = numpy.add(m2,a)\n",
    "    print(i)\n",
    "\n",
    "m3 /= 5\n",
    "print(all([numpy.allclose(x, y) for x, y in zip(m, m3)]))\n",
    "model3 = model_build()\n",
    "model3.set_weights(m3)\n",
    "loss, acc = model3.evaluate(x_test, y_test)\n",
    "print(acc)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 5, 1, 6, 4]\n",
      "9\n",
      "5\n",
      "1\n",
      "6\n",
      "4\n",
      "False\n",
      "634/634 [==============================] - 0s 247us/sample - loss: 0.6922 - acc: 0.4905\n",
      "0.49053627\n",
      "0.6921640822564014\n"
     ]
    }
   ],
   "source": [
    "#Selecting random 5 models\n",
    "import random\n",
    "list2 = []\n",
    "for i in range(0,5):\n",
    "    list2.append(random.randrange(0,10))\n",
    "print(list2)\n",
    "m3 = models[list2[0]].get_weights()\n",
    "for i in list2:\n",
    "    a = models[i].get_weights()\n",
    "    m3 = numpy.add(m2,a)\n",
    "    print(i)\n",
    "\n",
    "m3 /= 5\n",
    "print(all([numpy.allclose(x, y) for x, y in zip(m, m3)]))\n",
    "model3 = model_build()\n",
    "model3.set_weights(m3)\n",
    "loss, acc = model3.evaluate(x_test, y_test)\n",
    "print(acc)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_population = []\n",
    "\n",
    "for i in range(10):\n",
    "    new_population.append(models[i].get_weights())\n",
    "\n",
    "\"\"\"\n",
    "Genetic algorithm parameters:\n",
    "    Mating pool size\n",
    "    Population size\n",
    "\"\"\"\n",
    "sol_per_pop = 10\n",
    "num_parents_mating = int(sol_per_pop/2)\n",
    "best_outputs = []\n",
    "average_outputs = []\n",
    "num_generations = 5\n",
    "no_of_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def cal_pop_fitness(x_input, y_output, pop):\n",
    "    # Calculating the fitness value of each solution in the current population.\n",
    "    # The fitness function calulates the sum of products between each input and its corresponding weight.\n",
    "    fitness = []\n",
    "    for m in pop:\n",
    "        model = model_build()\n",
    "        model.set_weights(m)\n",
    "        loss, acc = model.evaluate(x_input, y_output)\n",
    "        fitness.append(acc) \n",
    "    return fitness\n",
    "\n",
    "def select_mating_pool(pop, fitness, num_parents):\n",
    "    # Selecting the best individuals in the current generation as parents for producing the offspring \n",
    "    # of the next generation.\n",
    "    parents = []\n",
    "    print(type(parents))\n",
    "    for parent_num in range(num_parents):\n",
    "        max_fitness_idx = fitness.index(max(fitness))\n",
    "        parents.append(pop[max_fitness_idx])\n",
    "        fitness[max_fitness_idx] = -99999999999\n",
    "    return parents\n",
    "\n",
    "def crossover(parents, offspring_size):\n",
    "    offspring_list = []\n",
    "    # The point at which crossover takes place between two parents. Usually, it is at the center.\n",
    "    crossover_point = int(offspring_size[1]/2)\n",
    "\n",
    "    for k in range(offspring_size[0]):\n",
    "        # Index of the first parent to mate.\n",
    "        parent1_idx = k%len(parents)\n",
    "        # Index of the second parent to mate.\n",
    "        parent2_idx = (k+1)%len(parents)\n",
    "        offspring1 = copy.deepcopy(parents[parent1_idx])\n",
    "        #Crossing odd numbered weight of Parent2 with Parent1\n",
    "        p = 0\n",
    "        for i,layer in enumerate(offspring1):\n",
    "            if i < no_of_layers-1:\n",
    "                for j,node in enumerate(layer):\n",
    "                        for k,weight in enumerate(node):\n",
    "                            if(p%2 == 1):\n",
    "                                offspring1[i][j][k] = parents[parent2_idx][i][j][k]\n",
    "                            p = p+1\n",
    "        offspring_list.append(offspring1)\n",
    "    return offspring_list\n",
    "\n",
    "def mutation(offspring_crossover, num_mutations):\n",
    "    # Mutation changes a number of genes as defined by the num_mutations argument. The changes are random.\n",
    "    for idx in range(len(offspring_crossover)):\n",
    "        for mutation_num in range(num_mutations):\n",
    "            layer_idx = random.randint(0, 1)\n",
    "            node_idx = random.randint(0, 9)\n",
    "            weight_idx = random.randint(0, 9)\n",
    "            # The random value to be added to the gene.\n",
    "            random_value = numpy.random.uniform(-1.0, 1.0, 1)/10\n",
    "            offspring_crossover[idx][layer_idx][node_idx][weight_idx]= offspring_crossover[idx][layer_idx][node_idx][weight_idx] + random_value\n",
    "    return offspring_crossover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation :  0\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.6836 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 4ms/sample - loss: 0.6506 - acc: 0.6498\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.6867 - acc: 0.5300\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.6862 - acc: 0.5063\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.8698 - acc: 0.4905\n",
      "634/634 [==============================] - 2s 4ms/sample - loss: 0.6867 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 4ms/sample - loss: 0.6949 - acc: 0.5016\n",
      "634/634 [==============================] - 2s 4ms/sample - loss: 0.7397 - acc: 0.4558\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.7155 - acc: 0.6262\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.6765 - acc: 0.5000\n",
      "Fitness\n",
      "[0.52681386, 0.64984226, 0.52996844, 0.50630915, 0.49053627, 0.5252366, 0.50157726, 0.45583597, 0.626183, 0.5]\n",
      "Best result :  0.64984226\n",
      "<class 'list'>\n",
      "Parents\n",
      "Crossover\n",
      "Mutation\n",
      "Generation :  1\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.6506 - acc: 0.6498\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.7155 - acc: 0.6262\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5300\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6836 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.0298 - acc: 0.4811\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.2318 - acc: 0.5174\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.1777 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 2.7223 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 5.1927 - acc: 0.5126\n",
      "Fitness\n",
      "[0.64984226, 0.626183, 0.52996844, 0.52681386, 0.5252366, 0.48107255, 0.51735014, 0.5252366, 0.5252366, 0.5126183]\n",
      "Best result :  0.64984226\n",
      "<class 'list'>\n",
      "Parents\n",
      "Crossover\n",
      "Mutation\n",
      "Generation :  2\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6506 - acc: 0.6498\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.7155 - acc: 0.6262\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5300\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6836 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.2946 - acc: 0.4890\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.2079 - acc: 0.5158\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.1103 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 2.7645 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 5.9011 - acc: 0.5221\n",
      "Fitness\n",
      "[0.64984226, 0.626183, 0.52996844, 0.52681386, 0.5252366, 0.48895898, 0.5157729, 0.5252366, 0.5252366, 0.52208203]\n",
      "Best result :  0.64984226\n",
      "<class 'list'>\n",
      "Parents\n",
      "Crossover\n",
      "Mutation\n",
      "Generation :  3\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6506 - acc: 0.6498\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.7155 - acc: 0.6262\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5300\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6836 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.9283 - acc: 0.5047\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.2760 - acc: 0.5174\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.8462 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 2.6748 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 5.1587 - acc: 0.5174\n",
      "Fitness\n",
      "[0.64984226, 0.626183, 0.52996844, 0.52681386, 0.5252366, 0.50473183, 0.51735014, 0.52681386, 0.5252366, 0.51735014]\n",
      "Best result :  0.64984226\n",
      "<class 'list'>\n",
      "Parents\n",
      "Crossover\n",
      "Mutation\n",
      "Generation :  4\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6506 - acc: 0.6498\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.7155 - acc: 0.6262\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5300\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6836 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.8462 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.9959 - acc: 0.4968\n",
      "634/634 [==============================] - 2s 4ms/sample - loss: 1.1711 - acc: 0.5237\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.0455 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 4ms/sample - loss: 0.7284 - acc: 0.4763\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 2.7110 - acc: 0.5252\n",
      "Fitness\n",
      "[0.64984226, 0.626183, 0.52996844, 0.52681386, 0.52681386, 0.49684542, 0.5236593, 0.5252366, 0.47634068, 0.5252366]\n",
      "Best result :  0.64984226\n",
      "<class 'list'>\n",
      "Parents\n",
      "Crossover\n",
      "Mutation\n"
     ]
    }
   ],
   "source": [
    "for generation in range(num_generations):\n",
    "    print(\"Generation : \", generation)\n",
    "    # Measuring the fitness of each chromosome in the population.\n",
    "    fitness = cal_pop_fitness(x_test, y_test, new_population)\n",
    "    print(\"Fitness\")\n",
    "    print(fitness)\n",
    "\n",
    "    best_outputs.append(max(fitness))\n",
    "    average_outputs.append(numpy.sum(fitness)/sol_per_pop)\n",
    "    # The best result in the current iteration.\n",
    "    print(\"Best result : \", max(fitness))\n",
    "    \n",
    "    # Selecting the best parents in the population for mating.\n",
    "    parents = select_mating_pool(new_population, fitness, \n",
    "                                      num_parents_mating)\n",
    "    print(\"Parents\")\n",
    "    #print(parents)\n",
    "\n",
    "    # Generating next generation using crossover.\n",
    "    offspring_crossover = crossover(parents,\n",
    "                                       offspring_size=((sol_per_pop - len(parents), no_of_layers)))\n",
    "    print(\"Crossover\")\n",
    "    #print(offspring_crossover)\n",
    "\n",
    "    # Adding some variations to the offspring using mutation.\n",
    "    offspring_mutation = mutation(offspring_crossover, num_mutations=10)\n",
    "    #offspring_mutation = offspring_crossover\n",
    "    print(\"Mutation\")\n",
    "    #print(offspring_mutation)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "    new_population = []\n",
    "    for parent in parents:\n",
    "        new_population.append(parent)\n",
    "    for offspring in offspring_mutation:\n",
    "        new_population.append(offspring)\n",
    "    #new_population = random.shuffle(new_population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6506 - acc: 0.6498\n",
      "634/634 [==============================] - 3s 4ms/sample - loss: 0.7155 - acc: 0.6262\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6867 - acc: 0.5300\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.6836 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.8462 - acc: 0.5268\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.9800 - acc: 0.4811\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.1728 - acc: 0.5221\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 1.0805 - acc: 0.5252\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 0.8745 - acc: 0.5047\n",
      "634/634 [==============================] - 2s 3ms/sample - loss: 2.3268 - acc: 0.5252\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-e8de12b2e163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Then return the index of that solution corresponding to the best fitness.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbest_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0maverage_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msol_per_pop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best solution fitness : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# Getting the best solution after iterating finishing all generations.\n",
    "#At first, the fitness is calculated for each solution in the final generation.\n",
    "fitness = cal_pop_fitness(x_test, y_test, new_population)\n",
    "# Then return the index of that solution corresponding to the best fitness.\n",
    "best_outputs.append(max(fitness))\n",
    "average_outputs.append(numpy.sum(fitness)/sol_per_pop)\n",
    "\n",
    "print(\"Best solution fitness : \", max(fitness))\n",
    "\n",
    "x = [0,1,2,3,4,5,6]\n",
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.plot(x, best_outputs)\n",
    "matplotlib.pyplot.plot(x, average_outputs)\n",
    "matplotlib.pyplot.xlabel(\"Generation\")\n",
    "matplotlib.pyplot.ylabel(\"Fitness\")\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution fitness :  0.64984226\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (8,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-d44bb39f45fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Generation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitness\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2787\u001b[0m     return gca().plot(\n\u001b[0;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2789\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m         \"\"\"\n\u001b[0;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1666\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1667\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 270\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (8,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best solution fitness : \", max(fitness))\n",
    "\n",
    "x = [0,1,2,3,4,5,6,8]\n",
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.plot(x, best_outputs)\n",
    "matplotlib.pyplot.plot(x, average_outputs)\n",
    "matplotlib.pyplot.xlabel(\"Generation\")\n",
    "matplotlib.pyplot.ylabel(\"Fitness\")\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
